{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Perceptron de Múltiplas Camadas\n",
    "&nbsp;\n",
    "\n",
    "**Questão 2** - Implemente uma rede perceptron de múltiplas camadas e utilize-a para aproximar as duas\n",
    "funções abaixo. Em seguida, compare os resultados com as curvas exatas. No caso da letra\n",
    "(b), apresente também a curva do erro médio de treinamento com relação ao número de\n",
    "épocas e a curva do erro médio com o conjunto de validação.\n",
    "\n",
    "**a)** Função lógica XOR <br>\n",
    "**b)** _f(x) = sen(πx) / πx_ , 0 ≤ x ≤ 4\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) _f(x) = sen(πx) / πx_ , 0 ≤ x ≤ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.37768741 3.03181761 1.68228632 1.035667   2.04509889 1.61973655\n",
      " 3.13519436 1.2132509  1.90638782 2.33352816 3.63245154 2.01874742\n",
      " 1.12735138 3.02321682 2.47347599 1.00202537 3.63898502 3.9311419\n",
      " 3.24086894 3.6086638  1.24059028 2.91932699 3.59535315 2.73593573\n",
      " 1.88857086 0.40280483 1.73668734 2.44354789 3.65204421 3.86642547\n",
      " 1.90803911 3.46123971 1.04196924 3.22011131 2.19479722 0.0561668\n",
      " 2.87881875 1.59529417 3.29937991 2.6726128  0.00457128 1.97431147\n",
      " 3.4704111  0.97564351 1.30081745 3.48188493 0.76426837 2.27004296\n",
      " 0.95446371 3.870161   3.21271788 1.79187829 0.32178327 1.28021842\n",
      " 2.03176257 3.7313353  0.43623138 2.20506898 2.82624564 2.18976365\n",
      " 3.25786745 2.16113443 3.85535418 2.41274251 2.35046826 1.77995611\n",
      " 2.38514745 1.53960458 2.30260406 1.16131801 0.75756531 0.74691811\n",
      " 2.45109272 2.62663756 1.90612397 0.35929744 3.03041569 3.50708148\n",
      " 3.69352406 3.36984089 3.59269249 3.69232976 2.1623997  1.5651842\n",
      " 2.8211336  1.10253649 3.24651483 3.39794386 3.58015587 2.35920473\n",
      " 3.79905949 2.31878004 1.80225243 2.64098151 3.98503136 3.66776487\n",
      " 3.17330034 0.32949195 2.45113242 1.94577681 2.52058936 3.3803103\n",
      " 0.97214249 2.92595688 0.46853717 0.88184215 3.17833189 1.3301446\n",
      " 3.26365239 0.40243008 0.58543396 2.79068256 0.18093627 2.29546415\n",
      " 3.64006406 2.13679187 2.72235653 0.10678718 2.53999964 2.42535367\n",
      " 2.30381179 1.56483764 1.48055976 3.9220666  0.14556815 0.08654604\n",
      " 3.84412512 0.73988777 0.49558066 0.84230604 3.20298636 3.74787663\n",
      " 0.0911303  1.70247533 0.40600088 1.03967956 0.88331709 2.58770288\n",
      " 1.40117587 0.72127161 2.01454602 0.15751483 0.40368496 3.95294059\n",
      " 0.79742316 1.43422121 2.92639322 3.35330626 3.67392825 0.67769842\n",
      " 2.69056225 3.86619561 0.23220378 2.70480714 3.38169837 1.36925016\n",
      " 1.00274936 2.38716557 1.76925613 0.69927794 1.88650166 1.63962158\n",
      " 2.27645096 2.03440052 1.245784   1.42860673 3.3506447  1.00373066\n",
      " 2.24240088 0.04974528 2.96629751 1.34366622 0.18278597 1.12353266\n",
      " 0.96052163 3.81251736 1.40890225 1.15151166 1.43680479 3.78762334\n",
      " 2.53499141 2.48430738 2.8624774  1.55206894 1.65767195 2.60333145\n",
      " 0.00609689 0.76923816 1.33760676 0.95766384 2.5495976  1.51459228\n",
      " 3.50169357 2.27260568 1.65762559 1.6090683  2.8073185  1.67290621\n",
      " 2.64878356 0.18711874 1.78140876 1.03690769 0.63074629 2.11029252\n",
      " 1.9490624  2.2456197  3.02193907 3.53550062 1.97833068 1.24823299\n",
      " 1.86756894 3.23618343 3.50006533 3.24965973 0.75200518 3.99768144\n",
      " 2.53235504 0.3338682  2.90221742 3.94728592 1.60726729 2.71406002\n",
      " 1.26470855 0.85409865 2.86929657 0.00943026 3.29092564 2.11338391\n",
      " 0.39113737 0.47561558 2.5970617  3.4946153  1.11993097 3.91406075\n",
      " 0.40072276 3.41575244 1.58678471 0.32538167 1.09885537 1.81191274\n",
      " 3.16936612 3.44543961 0.53368222 2.08346211 2.60313295 1.38821206\n",
      " 3.48745534 1.11363926 0.07429731 0.16265309 2.72398708 2.23342294\n",
      " 3.78601022 3.7537552  3.63940471 0.16801813 2.99653929 2.80529927\n",
      " 2.62144746 2.84943061 3.6108406  2.5605648  1.48979705 2.15171513\n",
      " 0.83137641 2.34850202 0.03558833 0.6040927  1.33363355 3.15849264\n",
      " 2.87399769 1.35302388 2.48215243 0.1648118  0.65544218 3.92765628\n",
      " 1.15812341 1.57916793 2.19393719 1.17362801 1.91225868 0.95882433\n",
      " 0.19302545 0.7183474  2.09220093 0.28345154 1.61267659 1.31408284\n",
      " 1.65888644 0.39760135 3.63463022 1.8960186  3.36339333 3.90491783\n",
      " 1.37460637 1.91634608 2.79838116 1.70614129 1.20761246 2.93900396\n",
      " 3.57759911 3.67875538 2.50696819 1.50228539 3.89824209 2.55551407\n",
      " 0.26333871 0.33867828 2.99947829 0.24462463 0.03140402 1.57523181\n",
      " 2.07601491 1.79417714 1.95447522 2.33955481 2.71721027 1.69215229\n",
      " 1.47332583 3.95383623 1.04366614 3.10840062 1.7248841  1.43408153\n",
      " 0.2554318  3.45431578 2.8080166  3.61204283 1.80644717 2.70768387\n",
      " 0.47564115 1.59181441 0.82892789 0.16840571 3.79184541 0.86357747\n",
      " 0.58541796 0.79188017 1.51212786 2.18556505 0.60533747 3.95475956\n",
      " 3.93195684 0.59360807 1.62362753 2.71971793 3.51062633 1.9816237\n",
      " 3.66818669 1.28984126 1.99376357 1.99458637 2.68027261 0.80796524\n",
      " 2.43908244 0.87509239 1.36088126 3.85026585 3.59603215 3.27247352\n",
      " 0.14187305 0.59346753 1.02752765 3.13666627 3.36933331 2.33179272\n",
      " 2.87252661 3.22822152 0.26543652 0.33857255 3.47558126 0.15766332\n",
      " 0.90036261 0.16252811 0.06114056 3.37581874 1.32237747 0.64276024\n",
      " 0.59527796 2.62433465 3.87439309 2.01999877 3.60436191 2.0097144\n",
      " 2.29548991 2.71428543 3.22043996 3.03138553 3.96213025 2.98786156\n",
      " 3.62312289 0.82441933 2.14166522 2.39445705 3.30278647 1.92885425\n",
      " 3.16416085 1.55427556 2.34555382 3.40526643]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "X= np.loadtxt('data_x.txt', dtype=float,  encoding='bytes')\n",
    "y= np.loadtxt('data_y.txt', dtype=float,  encoding='bytes')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.58541796 2.35046826 3.7313353  3.17833189 3.57759911 0.47561558\n 1.48055976 0.16252811 1.92885425 3.45431578 3.87439309 3.7537552\n 0.43623138 1.90638782 2.5495976  2.92639322 1.03967956 3.62312289\n 0.83137641 3.36933331 2.72398708 2.41274251 2.07601491 0.69927794\n 0.7183474  0.60533747 3.69232976 1.28984126 2.70480714 1.91634608\n 3.17330034 2.35920473 2.79838116 2.91932699 2.33179272 2.53235504\n 2.84943061 3.870161   1.09885537 0.59360807 1.80225243 0.85409865\n 0.40072276 1.11993097 0.64276024 0.40368496 3.59535315 3.78601022\n 1.99376357 2.24240088 2.69056225 2.39445705 2.38514745 3.16936612\n 2.90221742 3.3803103  1.57523181 1.65762559 2.8080166  3.9311419\n 1.245784   0.00609689 1.90803911 3.64006406 2.15171513 2.92595688\n 3.59603215 1.38821206 3.41575244 2.72235653 0.47564115 2.30381179\n 0.39760135 0.18093627 3.99768144 0.88331709 3.92765628 3.84412512\n 2.64878356 2.93900396 0.95882433 1.70247533 3.22011131 0.59527796\n 2.27645096 2.19479722 3.4946153  0.32949195 1.10253649 2.16113443\n 1.00373066 0.19302545 1.78140876 0.82441933 2.5970617  1.31408284\n 3.26365239 0.76426837 1.04366614 3.16416085 2.86929657 2.62663756\n 2.58770288 2.01999877 1.60726729 1.86756894 3.86642547 2.48430738\n 0.14187305 1.35302388 1.79187829 1.80644717 2.68027261 2.44354789\n 1.68228632 0.3338682  1.00274936 3.50006533 1.32237747 2.71406002\n 1.40117587 1.58678471 3.94728592 0.65544218 2.19393719 0.33867828\n 1.81191274 0.0561668  1.9490624  2.14166522 1.53960458 1.88857086\n 0.82892789 1.30081745 3.44543961 0.84230604 2.64098151 2.79068256\n 3.3506447  2.98786156 2.0097144  3.93195684 0.00457128 3.24086894\n 2.09220093 3.36984089 0.16265309 3.30278647 0.74691811 3.02321682\n 3.22822152 3.24965973 2.71428543 3.96213025 0.39113737 1.88650166\n 0.46853717 1.63962158 1.5651842  0.95766384 1.55427556 2.53999964\n 3.63940471 1.33760676 1.97833068 3.37581874 0.58543396 0.67769842\n 2.38716557 0.23220378 3.63898502 3.15849264 0.35929744 0.40243008\n 3.61204283 2.2456197  2.33955481 0.72127161 0.32538167 3.95383623\n 0.07429731 2.99947829 3.6086638  3.29092564 0.26333871 1.035667\n 1.15812341 0.08654604 3.50708148 1.65767195 3.67875538 2.62433465\n 3.51062633 2.8624774  1.36088126 3.59269249 2.62144746 2.01874742\n 1.20761246 3.24651483 0.03558833 2.87881875 3.60436191 2.82624564\n 1.97431147 2.87399769 3.21271788 1.24823299 1.69215229 2.71721027\n 1.59181441 3.9220666  1.26470855 3.85535418 2.50696819 3.20298636\n 0.76923816 2.73593573 0.97564351 3.37768741 1.03690769 2.18556505\n 2.45113242 3.03138553 1.43680479 2.99653929 3.98503136 0.33857255\n 3.66776487 1.99458637 0.04974528 1.16131801 2.55551407 0.95446371\n 3.53550062 1.17362801 2.11338391 3.67392825 3.95294059 2.53499141\n 3.74787663 3.85026585 2.34555382 1.48979705 0.2554318  2.11029252\n 2.8211336  1.56483764 2.71971793 2.87252661 2.31878004 2.1623997\n 0.6040927  2.42535367 1.8960186  2.20506898 3.10840062 2.80529927\n 0.87509239 3.4704111  0.88184215 0.26543652 0.1648118  0.80796524\n 3.29937991 1.28021842 1.9816237  0.49558066 3.63463022 3.65204421\n 1.55206894 2.03440052 3.86619561 2.60313295 0.63074629 3.46123971\n 1.04196924 0.73988777 2.60333145 0.90036261 1.65888644 1.02752765\n 3.35330626 0.28345154 3.47558126 1.15151166 1.94577681 1.51212786\n 0.86357747 1.67290621 2.08346211 2.13679187 2.34850202 2.45109272\n 0.16840571 0.40280483 1.42860673 0.79188017 2.29548991 0.96052163\n 3.22043996 2.6726128  2.27260568 1.95447522 3.58015587 0.75756531\n 3.39794386 3.36339333 0.53368222 1.57916793 3.23618343 2.33352816\n 3.27247352 1.6090683  2.23342294 1.43408153 3.50169357 0.10678718\n 2.27004296 0.18278597].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c2ac09642c45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[0mTransformer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \"\"\"\n\u001b[1;32m--> 696\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                                 force_all_finite='allow-nan')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    621\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.58541796 2.35046826 3.7313353  3.17833189 3.57759911 0.47561558\n 1.48055976 0.16252811 1.92885425 3.45431578 3.87439309 3.7537552\n 0.43623138 1.90638782 2.5495976  2.92639322 1.03967956 3.62312289\n 0.83137641 3.36933331 2.72398708 2.41274251 2.07601491 0.69927794\n 0.7183474  0.60533747 3.69232976 1.28984126 2.70480714 1.91634608\n 3.17330034 2.35920473 2.79838116 2.91932699 2.33179272 2.53235504\n 2.84943061 3.870161   1.09885537 0.59360807 1.80225243 0.85409865\n 0.40072276 1.11993097 0.64276024 0.40368496 3.59535315 3.78601022\n 1.99376357 2.24240088 2.69056225 2.39445705 2.38514745 3.16936612\n 2.90221742 3.3803103  1.57523181 1.65762559 2.8080166  3.9311419\n 1.245784   0.00609689 1.90803911 3.64006406 2.15171513 2.92595688\n 3.59603215 1.38821206 3.41575244 2.72235653 0.47564115 2.30381179\n 0.39760135 0.18093627 3.99768144 0.88331709 3.92765628 3.84412512\n 2.64878356 2.93900396 0.95882433 1.70247533 3.22011131 0.59527796\n 2.27645096 2.19479722 3.4946153  0.32949195 1.10253649 2.16113443\n 1.00373066 0.19302545 1.78140876 0.82441933 2.5970617  1.31408284\n 3.26365239 0.76426837 1.04366614 3.16416085 2.86929657 2.62663756\n 2.58770288 2.01999877 1.60726729 1.86756894 3.86642547 2.48430738\n 0.14187305 1.35302388 1.79187829 1.80644717 2.68027261 2.44354789\n 1.68228632 0.3338682  1.00274936 3.50006533 1.32237747 2.71406002\n 1.40117587 1.58678471 3.94728592 0.65544218 2.19393719 0.33867828\n 1.81191274 0.0561668  1.9490624  2.14166522 1.53960458 1.88857086\n 0.82892789 1.30081745 3.44543961 0.84230604 2.64098151 2.79068256\n 3.3506447  2.98786156 2.0097144  3.93195684 0.00457128 3.24086894\n 2.09220093 3.36984089 0.16265309 3.30278647 0.74691811 3.02321682\n 3.22822152 3.24965973 2.71428543 3.96213025 0.39113737 1.88650166\n 0.46853717 1.63962158 1.5651842  0.95766384 1.55427556 2.53999964\n 3.63940471 1.33760676 1.97833068 3.37581874 0.58543396 0.67769842\n 2.38716557 0.23220378 3.63898502 3.15849264 0.35929744 0.40243008\n 3.61204283 2.2456197  2.33955481 0.72127161 0.32538167 3.95383623\n 0.07429731 2.99947829 3.6086638  3.29092564 0.26333871 1.035667\n 1.15812341 0.08654604 3.50708148 1.65767195 3.67875538 2.62433465\n 3.51062633 2.8624774  1.36088126 3.59269249 2.62144746 2.01874742\n 1.20761246 3.24651483 0.03558833 2.87881875 3.60436191 2.82624564\n 1.97431147 2.87399769 3.21271788 1.24823299 1.69215229 2.71721027\n 1.59181441 3.9220666  1.26470855 3.85535418 2.50696819 3.20298636\n 0.76923816 2.73593573 0.97564351 3.37768741 1.03690769 2.18556505\n 2.45113242 3.03138553 1.43680479 2.99653929 3.98503136 0.33857255\n 3.66776487 1.99458637 0.04974528 1.16131801 2.55551407 0.95446371\n 3.53550062 1.17362801 2.11338391 3.67392825 3.95294059 2.53499141\n 3.74787663 3.85026585 2.34555382 1.48979705 0.2554318  2.11029252\n 2.8211336  1.56483764 2.71971793 2.87252661 2.31878004 2.1623997\n 0.6040927  2.42535367 1.8960186  2.20506898 3.10840062 2.80529927\n 0.87509239 3.4704111  0.88184215 0.26543652 0.1648118  0.80796524\n 3.29937991 1.28021842 1.9816237  0.49558066 3.63463022 3.65204421\n 1.55206894 2.03440052 3.86619561 2.60313295 0.63074629 3.46123971\n 1.04196924 0.73988777 2.60333145 0.90036261 1.65888644 1.02752765\n 3.35330626 0.28345154 3.47558126 1.15151166 1.94577681 1.51212786\n 0.86357747 1.67290621 2.08346211 2.13679187 2.34850202 2.45109272\n 0.16840571 0.40280483 1.42860673 0.79188017 2.29548991 0.96052163\n 3.22043996 2.6726128  2.27260568 1.95447522 3.58015587 0.75756531\n 3.39794386 3.36339333 0.53368222 1.57916793 3.23618343 2.33352816\n 3.27247352 1.6090683  2.23342294 1.43408153 3.50169357 0.10678718\n 2.27004296 0.18278597].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=3, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=5, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.fit(X_train, y_train, validation_split=0.2, epochs=400, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
